# Äá»“ Ã¡n Xá»­ lÃ½ NgÃ´n ngá»¯ Tá»± nhiÃªn (NLP) - HK1 2025-2026 ğŸ“

## ğŸ“– Äá» tÃ i: Dá»‹ch mÃ¡y Anh-PhÃ¡p / Anh-Äá»©c (Machine Translation)
**MÃ´ hÃ¬nh:** Sequence-to-Sequence (Seq2Seq) sá»­ dá»¥ng Encoder-Decoder LSTM.

---

## ğŸ‘¥ ThÃ nh viÃªn thá»±c hiá»‡n

| STT | Há» vÃ  tÃªn | MSSV |
|:---:|:---|:---:|
| 1 | **Mai Thanh Duy** | 3123410055 | 
| 2 | **Nguyá»…n Tuáº¥n Äáº¡t** | 3123410070 |

---

## ğŸ“ Giá»›i thiá»‡u (Introduction)

[cite_start]Dá»± Ã¡n nÃ y triá»ƒn khai má»™t mÃ´ hÃ¬nh **Sequence-to-Sequence (Seq2Seq)** sá»­ dá»¥ng kiáº¿n trÃºc **LSTM (Long Short-Term Memory)** Ä‘á»ƒ giáº£i quyáº¿t bÃ i toÃ¡n dá»‹ch mÃ¡y tá»« tiáº¿ng Anh sang tiáº¿ng PhÃ¡p[cite: 11].

### YÃªu cáº§u ká»¹ thuáº­t báº¯t buá»™c:
* [cite_start]**Kiáº¿n trÃºc:** Encoder-Decoder vá»›i context vector cá»‘ Ä‘á»‹nh[cite: 12].
* **RÃ ng buá»™c:** KhÃ´ng sá»­ dá»¥ng cÆ¡ cháº¿ Attention. [cite_start]KhÃ´ng sá»­ dá»¥ng cÃ¡c thÆ° viá»‡n seq2seq cÃ³ sáºµn (nhÆ° `torchtext.legacy` hay `transformers`), mÃ´ hÃ¬nh Ä‘Æ°á»£c xÃ¢y dá»±ng "from scratch" báº±ng PyTorch[cite: 12, 19].
* [cite_start]**Má»¥c tiÃªu:** Hiá»ƒu rÃµ cÃ¡ch xá»­ lÃ½ dá»¯ liá»‡u chuá»—i, packing/padding vÃ  luá»“ng hoáº¡t Ä‘á»™ng cá»§a RNN/LSTM[cite: 21, 22].
---

## ğŸ“‚ Cáº¥u trÃºc thÆ° má»¥c

```bash
â”œâ”€â”€ data/                   # Chá»©a dá»¯ liá»‡u train/test
â”œâ”€â”€ checkpoints/            # Chá»©a best_model 
â”œâ”€â”€ notebooks/              # Jupyter Notebooks 
â”œâ”€â”€ README.md               # ThÃ´ng tin thÃ nh viÃªn, dá»± Ã¡n
